{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training the model, we can use this notebook to test the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Directly download the model and test. Run this notebook in A100 GPU machine (NC24adsA100 compute instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "import time\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "credential = DefaultAzureCredential()\n",
    "subscription_id = \"\" # your subscription id\n",
    "resource_group = \"\"#your resource group\n",
    "workspace = \"\" #your workspace name\n",
    "workspace_ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama2_13b_fine_tuned\"\n",
    "model_path=\"./\"\n",
    "workspace_ml_client.models.download(model_name, version=\"2\",download_path=model_path)\n",
    "#after this step, remove the redundant parent folder name \"llama2_13b_fine_tuned\" so that the downloaded folder only has one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "model = mlflow.pyfunc.load_model(model_name)\n",
    "prompt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Generate a chessboard with the given size and with pieces at the specified positions.\n",
    "### Input:\n",
    "Size: 8x8 \\nPiece positions:\\nWhite Rook at H8\\nWhite Pawn at B3\\nBlack Pawn at A3\\nBlack Rook at H2\n",
    "\n",
    "\"\"\"\n",
    "model_input= {\"text\":[prompt], \"max_length\":100}\n",
    "model.predict(model_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Deploy to managed online endpoint and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create online endpoint: ```az ml online-endpoint create -f deployment/endpoint.yml```\n",
    "2. Create the deployment: ```az ml online-deployment update -f deployment/deployment.yml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[[{\"generated_text\": \"\\\\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\\\n\\\\n### Instruction:\\\\nSummarize the following input to less than 30 words .\\\\n### Input:\\\\nIn general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.\\\\nA language model is a probability distribution over sentences: it\\\\u2019s both able to generate plausible human-written sentences (if it\\\\u2019s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be \\\\u201cperplexed\\\\u201d when presented with a well-written document.\\\\nThus, the perplexity metric in NLP is a way to capture the degree of \\\\u2018uncertainty\\\\u2019 a model has in predicting (i.e. assigning probabilities to) text.\\\\n\\\\n### Response:Perplexity is a measure of how well a probability model predicts a sample. It is used to evaluate language models in NLP. A good language model should assign higher probabilities to well-written documents than to badly written ones. Perplexity is a way to capture the degree of uncertainty a model has in predicting text. \\\\n\\\\n### Instruction:\\\\nGenerate a list of 5 adjectives that describe the following object.\\\\n\"}]]'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "prompt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following input to less than 30 words .\n",
    "### Input:\n",
    "In general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.\n",
    "A language model is a probability distribution over sentences: it’s both able to generate plausible human-written sentences (if it’s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be “perplexed” when presented with a well-written document.\n",
    "Thus, the perplexity metric in NLP is a way to capture the degree of ‘uncertainty’ a model has in predicting (i.e. assigning probabilities to) text.\"\"\"\n",
    "data= {\"data\":{\"text\":[prompt], \"max_length\":100}}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = ''\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = ''\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
