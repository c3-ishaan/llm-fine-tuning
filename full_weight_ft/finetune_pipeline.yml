$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

experiment_name: llma31-8B_full_fine_tuning
description: Llama-3.1-8B-full-fine-tuning
inputs:
  num_examples: 56000 #update this to the number of examples you want to use for fine tuning
  train_path: data/train.jsonl #update this to the path of your training data
  test_path: data/test.jsonl #update this to the path of your test data
  special_token_path: data/tokens.json #update this to the path of your special tokens file
  model_name: llama2_70b_fine_tuned
  chat_model: False #whether to use the chat model or base model. The format of input data is different between the two models
  model_dir: 
    path: azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B/versions/2
outputs:
  # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model
  # registering the model is required to deploy the model to an online or batch endpoint
  trained_model:
jobs:
  train:
    type: command
    code: ./
    command: >-
      python finetune_hf_llm.py
      --model_dir ${{inputs.model_dir}}
      --trained_model ${{outputs.output_dir}}
      --model_name ${{inputs.model_name}}
      --chat_model ${{inputs.chat_model}}
      --train_path ${{inputs.train_path}}
      --special_token_path ${{inputs.special_token_path}}
      --test_path ${{inputs.test_path}}

    inputs:
      epochs: 2
      num_examples: ${{parent.inputs.num_examples}}
      model_dir: ${{parent.inputs.model_dir}} 
      model_name: ${{parent.inputs.model_name}}
      chat_model: ${{parent.inputs.chat_model}}
      train_path: ${{parent.inputs.train_path}}
      test_path: ${{parent.inputs.test_path}}
      special_token_path: ${{parent.inputs.special_token_path}}
    environment: 
      conda_file: ./conda.yml
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1
    outputs:
      output_dir: ${{parent.outputs.trained_model}}
    compute: azureml:NC48adsA100 #update NC48adsA100 to the name of your compute target 

    distribution:
      type: pytorch
      process_count_per_instance: 2 #update this to the number of GPUs on your compute target
    resources:
      instance_count: 4 #update this to the number of nodes in your cluster
